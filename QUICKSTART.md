# ğŸš€ Quickstart Guide - IDFM Analytics DataOps

Step-by-step guide to run your first data pipeline and understand the project.

---

## ğŸ“– What is This Project?

This is a **production-grade data engineering project** that demonstrates:

### Business Problem
Analyze Paris public transport (IDFM) to answer:
- Which train lines are most punctual?
- Which stations have the highest ridership?
- How has ridership changed over time?
- Are there data quality issues in the reporting?

### Technical Solution
A modern ELT (Extract-Load-Transform) pipeline:
1. **Extract** data from French government open data APIs
2. **Load** raw JSON into BigQuery (data lake pattern)
3. **Transform** with dbt into dimensional model (star schema)
4. **Orchestrate** with Airflow for daily automation
5. **Monitor** data quality and pipeline health

### Skills Demonstrated
âœ… Data Engineering: API ingestion, batch processing  
âœ… Cloud Infrastructure: Google Cloud Platform (BigQuery, GCS)  
âœ… Data Modeling: Dimensional modeling (Kimball methodology)  
âœ… Modern Stack: dbt, Airflow, Docker, Python  
âœ… Best Practices: Testing, CI/CD, documentation, version control

---

## ğŸ“ Project Structure Explained

```
idfm-analytics-dataops/
â”‚
â”œâ”€â”€ ğŸ“‚ ingestion/                   # Extract: API â†’ JSON files
â”‚   â”œâ”€â”€ odsv2_client.py            # Reusable API client (pagination, retry)
â”‚   â”œâ”€â”€ extract_validations.py      # Fetch ticket validation data
â”‚   â”œâ”€â”€ extract_ponctuality.py      # Fetch train punctuality data
â”‚   â”œâ”€â”€ extract_ref_*.py            # Fetch reference data (stops, lines)
â”‚   â””â”€â”€ load_bigquery_raw.py        # Load: JSON â†’ BigQuery RAW tables
â”‚
â”œâ”€â”€ ğŸ“‚ warehouse/dbt/               # Transform: RAW â†’ Analytics
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/                # Clean & standardize (1:1 with source)
â”‚       â”œâ”€â”€ core/                   # Business logic (dimensions + facts)
â”‚       â””â”€â”€ marts/                  # Final analytics tables
â”‚
â”œâ”€â”€ ğŸ“‚ orchestration/airflow/       # Orchestrate: Automate the pipeline
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ transport_daily_pipeline.py   # Run every morning at 2am
â”‚   â”‚   â””â”€â”€ transport_backfill.py         # Historical data loading
â”‚   â””â”€â”€ docker-compose.yml          # Airflow containers definition
â”‚
â”œâ”€â”€ ğŸ“‚ config/                      # Configuration files
â”‚   â”œâ”€â”€ apis.yml                    # API endpoints & field mappings
â”‚   â””â”€â”€ tables.yml                  # BigQuery schemas
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                     # Utility scripts
â”‚   â”œâ”€â”€ setup_bigquery.py           # Create BigQuery datasets
â”‚   â””â”€â”€ check_sla.py                # Validate data quality SLAs
â”‚
â””â”€â”€ ğŸ“‚ tests/                       # Quality assurance
    â””â”€â”€ unit/                       # Unit tests for Python code
```

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IDFM API   â”‚  French govt open data APIs
â”‚  Transilien â”‚  (Opendatasoft platform)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Python extract_*.py
       â”‚ (with retry, pagination, rate limiting)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bronze/Raw  â”‚  JSON files (temporary storage)
â”‚ data/bronze â”‚  
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ load_bigquery_raw.py
       â”‚ (batch insert to BigQuery)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BigQuery    â”‚  Raw data (exact copy of API response)
â”‚ RAW dataset â”‚  Columns in French (preserves source)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ dbt models (SQL transformations)
       â”‚ - staging: clean & rename to English
       â”‚ - core: business logic, dimensional model
       â”‚ - marts: final analytics tables
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BigQuery    â”‚  Analytics-ready tables
â”‚ ANALYTICS   â”‚  âœ“ Dimensional model (star schema)
â”‚ dataset     â”‚  âœ“ English column names
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  âœ“ Tested & documented
```

---

## ğŸ¯ Running Your First Pipeline

### Prerequisites
âœ… You've completed [SETUP.md](SETUP.md)  
âœ… Airflow is running at http://localhost:8080  
âœ… You have GCP credentials configured

---

## Step 1: Extract Data (Bronze Layer)

**What**: Fetch raw data from APIs and save as JSON files

```bash
# Extract 1 week of ticket validations (ridership data)
python ingestion/extract_validations.py \
    --start 2024-01-01 \
    --end 2024-01-07 \
    --output data/bronze/validations

# Expected output:
# INFO - Fetching all records (max: unlimited)
# INFO - Total records available: 15234
# INFO - Fetched 15234/15234 records
# INFO - âœ… Saved 15234 records to data/bronze/validations/validations_2024-01-01_2024-01-07.json
```

**What happened?**
- Connected to IDFM Opendatasoft API
- Paginated through results (100 records per API call)
- Applied rate limiting (0.5s between requests)
- Saved to JSON: `data/bronze/validations/validations_2024-01-01_2024-01-07.json`

**Check the output:**
```bash
# View first record
head -20 data/bronze/validations/validations_2024-01-01_2024-01-07.json

# Should show structure like:
# {
#   "date": "2024-01-01",
#   "stop_name": "Chatelet",
#   "validation_count": 45231,
#   "ticket_category": "Navigo",
#   "ingestion_ts": "2024-02-03T10:30:00",
#   "source": "idfm_validations"
# }
```

### Extract Punctuality Data

```bash
# Extract punctuality for January 2024
python ingestion/extract_ponctuality.py \
    --start 2024-01-01 \
    --end 2024-01-31 \
    --output data/bronze/punctuality
```

### Extract Reference Data

```bash
# Stops (stations)
python ingestion/extract_ref_stops.py --output data/bronze/referentials

# Lines (e.g., RER A, Metro 1, etc.)
python ingestion/extract_ref_lines.py --output data/bronze/referentials

# Stop-Line mappings (which lines stop at which stations)
python ingestion/extract_ref_stop_lines.py --output data/bronze/referentials
```

---

## Step 2: Load to BigQuery RAW (Silver Layer)

**What**: Upload JSON files to BigQuery as raw tables

```bash
python ingestion/load_bigquery_raw.py

# Expected output:
# INFO - Loading validations_2024-01-01_2024-01-07.json
# INFO - âœ… Loaded 15234 rows to transport_raw.raw_validations
# INFO - Loading punctuality_2024-01-01_2024-01-31.json
# INFO - âœ… Loaded 248 rows to transport_raw.raw_punctuality
# INFO - âœ… All files loaded successfully
```

**What happened?**
- Read all JSON files from `data/bronze/`
- Inferred BigQuery schema from JSON structure
- Batch inserted to `transport_raw` dataset
- Tables created: `raw_validations`, `raw_punctuality`, `raw_ref_stops`, etc.

**Verify in BigQuery Console:**
```sql
-- Check row counts
SELECT 
    'validations' as dataset,
    COUNT(*) as row_count 
FROM `your-project.transport_raw.raw_validations`

UNION ALL

SELECT 
    'punctuality',
    COUNT(*) 
FROM `your-project.transport_raw.raw_punctuality`;
```

---

## Step 3: Transform with dbt (Gold Layer)

**What**: Clean, standardize, and model data into analytics-ready tables

```bash
cd warehouse/dbt

# Compile models (check SQL syntax)
dbt compile --target dev

# Run all transformations
dbt run --target dev

# Expected output:
# Running with dbt=1.7.4
# Found 12 models, 15 tests, 0 snapshots
#
# 15:30:00 | Staging models (5/5)
# 15:30:05 | Core models (5/5)
# 15:30:12 | Mart models (2/2)
#
# Completed successfully
```

**What happened?**

### Staging Layer (Clean & Standardize)
```sql
-- models/staging/stg_validations_rail_daily.sql
-- Renames French columns to English
-- Standardizes data types
-- Adds metadata

SELECT
    jour AS date,                      -- French â†’ English
    libelle_arret AS stop_name,
    id_refa_lda AS stop_id,
    nb_vald AS validation_count,
    categorie_titre AS ticket_category,
    ingestion_ts,
    source
FROM {{ source('raw', 'validations') }}
WHERE jour IS NOT NULL
```

### Core Layer (Business Logic)

**Dimensions** (who, what, where):
- `dim_stop`: All metro/train stations with coordinates
- `dim_line`: All transport lines with colors
- `dim_date`: Date dimension (day, month, year, weekday)
- `dim_ticket_type`: Types of tickets/passes

**Facts** (metrics, measures):
- `fct_validations_daily`: Daily ridership by stop/ticket type
- `fct_punctuality_monthly`: Monthly punctuality by line

### Marts Layer (Analytics)

**Business Mart**:
```sql
-- models/marts/business/mart_network_scorecard_monthly.sql
-- Executive dashboard: Network performance KPIs

SELECT
    p.month,
    l.line_name,
    p.punctuality_rate,
    v.total_validations,
    p.cancelled_trains / p.scheduled_trains AS cancellation_rate
FROM fct_punctuality_monthly p
JOIN dim_line l ON p.line_id = l.line_id
JOIN validations_agg v ON v.month = p.month
```

**Monitoring Mart**:
```sql
-- models/marts/monitoring/fct_data_health_daily.sql
-- Data quality checks

SELECT
    CURRENT_DATE() AS check_date,
    'validations' AS dataset_name,
    COUNT(*) AS row_count,
    CASE 
        WHEN COUNT(*) > 10000 THEN 'pass'
        WHEN COUNT(*) > 5000 THEN 'warn'
        ELSE 'fail'
    END AS check_status
FROM fct_validations_daily
WHERE date = CURRENT_DATE() - 1
```

**Verify transformations:**
```sql
-- Check marts
SELECT * FROM transport_analytics.mart_network_scorecard_monthly
ORDER BY month DESC
LIMIT 10;
```

---

## Step 4: Run Tests

**What**: Validate data quality with dbt tests

```bash
# Still in warehouse/dbt
dbt test --target dev

# Expected output:
# Running with dbt=1.7.4
# Found 15 tests
#
# 15:32:00 | Test not_null_stg_validations_date                   PASS
# 15:32:01 | Test unique_dim_stop_stop_id                         PASS
# 15:32:02 | Test relationships_fct_validations_stop_id           PASS
# ...
# 15:32:15 | Completed successfully
#
# Done. PASS=15 WARN=0 ERROR=0 SKIP=0 TOTAL=15
```

**What tests ran?**
- **Not null**: Critical fields have no nulls
- **Unique**: Primary keys are unique
- **Relationships**: Foreign keys exist in parent tables
- **Accepted values**: Enum fields have valid values

---

## Step 5: Check Data Quality (SLA)

**What**: Verify pipeline meets Service Level Agreements

```bash
cd ../..  # Back to project root
python scripts/check_sla.py

# Expected output:
# Checking SLA for 2024-01-07...
#
# âœ… Dataset: validations
#    Status: pass
#    Rows: 15234 (expected: > 10000)
#    Freshness: 2 hours (expected: < 24 hours)
#
# âœ… Dataset: punctuality
#    Status: pass
#    Rows: 31 (expected: > 20)
#
# âœ… Overall: ALL CHECKS PASSED
```

---

## Step 6: Run Full Pipeline with Airflow

**What**: Automate steps 1-5 with Airflow orchestration

### Via UI (Recommended):

1. Open http://localhost:8080
2. Find DAG: `transport_daily_pipeline`
3. Toggle **ON** (unpause)
4. Click **Trigger DAG** â–¶ï¸
5. Click DAG name â†’ **Graph** view
6. Watch tasks turn green âœ…

### Via CLI:

```bash
cd orchestration/airflow
docker exec -it airflow-scheduler airflow dags trigger transport_daily_pipeline

# Monitor logs
docker-compose logs -f airflow-scheduler
```

### DAG Structure:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    transport_daily_pipeline (DAG)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€ [EXTRACT] (parallel)
           â”‚   â”œâ”€ extract_validations
           â”‚   â”œâ”€ extract_punctuality
           â”‚   â””â”€ extract_referentials
           â”‚
           â”œâ”€ [LOAD]
           â”‚   â””â”€ load_bigquery_raw
           â”‚
           â”œâ”€ [TRANSFORM]
           â”‚   â””â”€ dbt_build (run + test)
           â”‚
           â”œâ”€ [VALIDATE]
           â”‚   â””â”€ check_sla
           â”‚
           â””â”€ [ALERT]
               â””â”€ notify_success (Slack)
```

**Execution time**: ~10-15 minutes for 1 week of data

---

## Step 7: Explore the Results

### Option 1: BigQuery Console

```sql
-- Top 10 stations by ridership (Jan 2024)
SELECT 
    s.stop_name,
    SUM(v.validation_count) AS total_validations
FROM transport_analytics.fct_validations_daily v
JOIN transport_core.dim_stop s ON v.stop_id = s.stop_id
WHERE v.date BETWEEN '2024-01-01' AND '2024-01-31'
GROUP BY s.stop_name
ORDER BY total_validations DESC
LIMIT 10;

-- Average punctuality by line (Jan 2024)
SELECT 
    l.line_name,
    AVG(p.punctuality_rate) AS avg_punctuality
FROM transport_analytics.fct_punctuality_monthly p
JOIN transport_core.dim_line l ON p.line_id = l.line_id
WHERE p.month = '2024-01'
GROUP BY l.line_name
ORDER BY avg_punctuality DESC;
```

### Option 2: dbt Documentation

```bash
cd warehouse/dbt
dbt docs generate --target dev
dbt docs serve

# Opens http://localhost:8080
# Browse data lineage, column descriptions, tests
```

### Option 3: Airflow Logs

View execution details, errors, and metrics in Airflow UI.

---

## ğŸ“ Understanding the Pipeline

### Why This Architecture?

**Bronze (Raw) Layer**
- âœ… Preserves original data for auditing
- âœ… Enables reprocessing without API calls
- âœ… Decouples ingestion from transformation

**Silver (Staging) Layer**
- âœ… Cleans data once, use many times
- âœ… Standardizes naming conventions
- âœ… Provides consistent data types

**Gold (Marts) Layer**
- âœ… Optimized for specific analytics use cases
- âœ… Pre-aggregated for performance
- âœ… Business-friendly naming

### Key Design Patterns

1. **Idempotency**: Running pipeline twice produces same result
2. **Incremental Loading**: Only process new data (not implemented in V1)
3. **Dimensional Modeling**: Star schema for fast queries
4. **Data Quality Tests**: Fail fast on bad data
5. **Monitoring**: SLA checks ensure data freshness

---

## ğŸ”„ Daily Operations

### Morning Check (If Automated)

```bash
# Check yesterday's pipeline run
# Airflow UI â†’ DAGs â†’ transport_daily_pipeline â†’ Recent Runs

# Or via CLI
docker exec -it airflow-scheduler \
    airflow dags list-runs -d transport_daily_pipeline --state success

# Expected: Daily run completed successfully
```

### Weekly Maintenance

```bash
# Re-run failed dates
make airflow-backfill START_DATE=2024-01-15 END_DATE=2024-01-21

# Update reference data (changes infrequently)
python ingestion/extract_ref_stops.py
python ingestion/extract_ref_lines.py
```

### Monthly Tasks

- Review BigQuery storage costs
- Check data quality trends in `fct_data_health_daily`
- Rotate GCP service account keys
- Update documentation

---

## ğŸ“Š Sample Queries for Analysis

```sql
-- 1. Weekday vs Weekend ridership
SELECT 
    d.is_weekend,
    AVG(v.validation_count) AS avg_validations
FROM fct_validations_daily v
JOIN dim_date d ON v.date = d.date
GROUP BY d.is_weekend;

-- 2. Most improved punctuality (YoY)
WITH this_year AS (
    SELECT line_id, AVG(punctuality_rate) AS rate
    FROM fct_punctuality_monthly
    WHERE month BETWEEN '2024-01' AND '2024-12'
    GROUP BY line_id
),
last_year AS (
    SELECT line_id, AVG(punctuality_rate) AS rate
    FROM fct_punctuality_monthly
    WHERE month BETWEEN '2023-01' AND '2023-12'
    GROUP BY line_id
)
SELECT 
    l.line_name,
    this_year.rate - last_year.rate AS improvement
FROM this_year
JOIN last_year ON this_year.line_id = last_year.line_id
JOIN dim_line l ON this_year.line_id = l.line_id
ORDER BY improvement DESC
LIMIT 5;

-- 3. Data quality over time
SELECT 
    check_date,
    dataset_name,
    check_status,
    row_count
FROM fct_data_health_daily
WHERE check_status != 'pass'
ORDER BY check_date DESC;
```

---

## ğŸš€ Next Steps

### Enhance the Pipeline

1. **Add Incremental Models**: Only process new/changed data
   ```sql
   {{
     config(
       materialized='incremental',
       unique_key='date'
     )
   }}
   ```

2. **Implement Great Expectations** (V2): Advanced data quality
3. **Add Looker/Tableau**: Business intelligence layer
4. **Set up Alerting**: Slack/email on failures
5. **Add CI/CD**: Automated testing on pull requests

### Portfolio Showcase

This project demonstrates:
- âœ… Modern data stack (dbt, Airflow, BigQuery)
- âœ… Engineering best practices (testing, documentation, version control)
- âœ… Production-ready code (error handling, logging, monitoring)
- âœ… Cloud infrastructure (GCP, Docker)
- âœ… Dimensional modeling (Kimball methodology)

---

## ğŸ“š Additional Resources

- **dbt Docs**: https://docs.getdbt.com
- **Airflow Docs**: https://airflow.apache.org/docs
- **BigQuery Docs**: https://cloud.google.com/bigquery/docs
- **Field Dictionary**: [docs/FIELD_DICTIONARY.md](docs/FIELD_DICTIONARY.md)

---

**Congratulations! You've run a complete data pipeline.** ğŸ‰
