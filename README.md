# ğŸš‡ IDFM Analytics DataOps

> Production-grade batch analytics platform for Paris public transport (IDFM network)

[![CI](https://github.com/your-username/idfm-analytics-dataops/workflows/CI/badge.svg)](https://github.com/your-username/idfm-analytics-dataops/actions)
[![dbt](https://img.shields.io/badge/dbt-1.7.4-orange.svg)](https://www.getdbt.com/)
[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/)
[![Airflow](https://img.shields.io/badge/airflow-2.8.1-blue.svg)](https://airflow.apache.org/)

A complete data engineering project demonstrating modern ELT pipeline with **dbt**, **Airflow**, and **BigQuery** to analyze Paris public transportation data from open government APIs.

---

## ğŸ“– What Does This Project Do?

Analyzes Paris public transport network (IDFM) to answer questions like:
- ğŸ“Š Which train lines are most/least punctual?
- ğŸš‰ Which stations have the highest ridership?
- ğŸ“ˆ How has ridership changed over time?
- âš ï¸ Are there data quality issues in public reporting?

### Business Value
Provides actionable insights for:
- Transport operations teams
- Urban planners
- Policy makers
- Data journalists
- Public

---

## ğŸ¯ Key Features

- âœ… **Modern Data Stack**: dbt + Airflow + BigQuery
- âœ… **Production-Ready**: Error handling, retry logic, monitoring
- âœ… **Best Practices**: Tests, CI/CD, documentation, version control
- âœ… **Dimensional Modeling**: Star schema (Kimball methodology)
- âœ… **Data Quality**: Automated SLA checks, dbt tests
- âœ… **Containerized**: Docker for local development
- âœ… **Open Data**: Uses public French government APIs

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IDFM/SNCF   â”‚  French government open data APIs
â”‚   APIs      â”‚  (Opendatasoft platform)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Python ingestion (pagination + retry)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bronze     â”‚  Raw JSON files (landing zone)
â”‚  Layer      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ load_bigquery_raw.py
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BigQuery    â”‚  Raw tables (French column names)
â”‚ RAW dataset â”‚  Preserves source structure
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ dbt transformations (SQL)
       â”œâ”€ Staging: Clean & standardize (English names)
       â”œâ”€ Core: Dimensional model (facts + dimensions)
       â””â”€ Marts: Analytics-ready tables
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BigQuery    â”‚  Star schema optimized for BI tools
â”‚ ANALYTICS   â”‚  âœ“ Tested âœ“ Documented âœ“ Monitored
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Explained

1. **Extract** (Python): Fetch from APIs â†’ Save JSON
2. **Load** (Python): JSON â†’ BigQuery RAW tables
3. **Transform** (dbt): RAW â†’ Staging â†’ Core â†’ Marts
4. **Validate** (dbt + custom): Run tests, check SLAs
5. **Orchestrate** (Airflow): Automate daily at 2 AM

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+, Docker, GCP account
- 45 minutes setup time

### 1. Setup Environment

```bash
git clone https://github.com/your-username/idfm-analytics-dataops.git
cd idfm-analytics-dataops

python -m venv venv
source venv/bin/activate
make install

cp .env.example .env
# Edit .env with your GCP credentials
```

### 2. Create GCP Resources

```bash
# Create service account and download key to credentials/
# Enable BigQuery API
# Create datasets
python scripts/setup_bigquery.py
```

### 3. Run Pipeline Locally

```bash
# Extract 1 week of data
python ingestion/extract_validations.py --start 2024-01-01 --end 2024-01-07
python ingestion/extract_ponctuality.py --start 2024-01-01 --end 2024-01-31

# Load to BigQuery
python ingestion/load_bigquery_raw.py

# Transform with dbt
cd warehouse/dbt
dbt run --target dev
dbt test --target dev
```

### 4. Start Airflow

```bash
cd orchestration/airflow
docker-compose up -d

# Access UI: http://localhost:8080
# Username: airflow, Password: airflow
```

**ğŸ“š Full documentation**: See [SETUP.md](SETUP.md) and [QUICKSTART.md](QUICKSTART.md)

---

## ğŸ“‚ Project Structure

```
idfm-analytics-dataops/
â”‚
â”œâ”€â”€ ğŸ“‚ ingestion/             # Extract & Load (Python)
â”‚   â”œâ”€â”€ odsv2_client.py       # API client with retry/pagination
â”‚   â”œâ”€â”€ extract_*.py          # Data extraction scripts
â”‚   â””â”€â”€ load_bigquery_raw.py  # Load JSON â†’ BigQuery
â”‚
â”œâ”€â”€ ğŸ“‚ warehouse/dbt/         # Transform (SQL)
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/          # Clean & standardize (1:1)
â”‚       â”œâ”€â”€ core/             # Dimensions + Facts
â”‚       â””â”€â”€ marts/            # Analytics-ready tables
â”‚
â”œâ”€â”€ ğŸ“‚ orchestration/airflow/ # Orchestrate
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ transport_daily_pipeline.py
â”‚   â”‚   â””â”€â”€ transport_backfill.py
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/               # Utilities
â”‚   â”œâ”€â”€ setup_bigquery.py     # Setup GCP
â”‚   â””â”€â”€ check_sla.py          # Data quality checks
â”‚
â”œâ”€â”€ ğŸ“‚ config/                # Configuration
â”‚   â”œâ”€â”€ apis.yml              # API endpoints & mappings
â”‚   â””â”€â”€ tables.yml            # BigQuery schemas
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                 # Quality assurance
â”‚   â””â”€â”€ unit/                 # Python unit tests
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                  # Documentation
â”‚   â””â”€â”€ FIELD_DICTIONARY.md   # French â†” English mappings
â”‚
â”œâ”€â”€ .env.example              # Environment template
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ Makefile                  # Developer commands
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ SETUP.md                  # Detailed setup guide
â””â”€â”€ QUICKSTART.md             # Step-by-step tutorial
```

---

## ğŸ’» Available Commands

```bash
# Installation
make install              # Install Python + dbt dependencies
make setup-gcp            # Create BigQuery datasets

# Development
make ingest               # Extract all data sources
make load-raw             # Load to BigQuery RAW
make dbt-build            # dbt run + test
make pipeline-daily       # Full local pipeline

# Airflow
make airflow-start        # Start Airflow containers
make airflow-stop         # Stop Airflow
make airflow-trigger-daily  # Manual DAG trigger

# Quality
make test                 # Run pytest
make lint                 # Check code quality
make format               # Auto-format code

# See all commands
make help
```

---

## ğŸ—‚ï¸ Data Model

### Dimensional Model (Star Schema)

**Fact Tables** (metrics):
- `fct_validations_daily`: Daily ticket validations by station
- `fct_punctuality_monthly`: Monthly punctuality rates by line

**Dimension Tables** (context):
- `dim_stop`: Stations/stops with coordinates
- `dim_line`: Transport lines with colors
- `dim_date`: Date dimension (day, month, year, weekday)
- `dim_ticket_type`: Ticket categories

**Mart Tables** (analytics):
- `mart_network_scorecard_monthly`: Executive KPI dashboard
- `fct_data_health_daily`: Data quality monitoring

### Field Naming

- **Raw Layer**: French (preserves source: `jour`, `ligne`, `nb_vald`)
- **Analytics Layer**: English (standardized: `date`, `line`, `validation_count`)

See [docs/FIELD_DICTIONARY.md](docs/FIELD_DICTIONARY.md) for complete mappings.

---

## ğŸ“Š Sample Queries

```sql
-- Top 10 busiest stations (January 2024)
SELECT 
    s.stop_name,
    SUM(v.validation_count) AS total_validations
FROM transport_analytics.fct_validations_daily v
JOIN transport_core.dim_stop s ON v.stop_id = s.stop_id
WHERE v.date BETWEEN '2024-01-01' AND '2024-01-31'
GROUP BY s.stop_name
ORDER BY total_validations DESC
LIMIT 10;

-- Average punctuality by line
SELECT 
    l.line_name,
    AVG(p.punctuality_rate) AS avg_punctuality,
    AVG(p.cancelled_trains * 100.0 / p.scheduled_trains) AS cancellation_rate
FROM transport_analytics.fct_punctuality_monthly p
JOIN transport_core.dim_line l ON p.line_id = l.line_id
WHERE p.month = '2024-01'
GROUP BY l.line_name
ORDER BY avg_punctuality DESC;
```

---

## ğŸ§ª Testing Strategy

### Python Tests (pytest)
```bash
pytest tests/ -v --cov=ingestion
```
- Unit tests for API client
- Integration tests for extraction scripts

### dbt Tests
```bash
cd warehouse/dbt
dbt test --target dev
```
- Not null on critical fields
- Unique primary keys
- Referential integrity (foreign keys)
- Accepted values for enums

### Data Quality (SLA Checks)
```bash
python scripts/check_sla.py
```
- Freshness: Data available within 24h
- Completeness: Expected row counts
- Validity: No nulls in required fields

---

## ğŸš€ Deployment

### CI/CD Pipeline (GitHub Actions)

**On Pull Request**:
1. Lint Python (black, isort, flake8)
2. Lint SQL (sqlfluff)
3. Run pytest
4. Compile dbt models

**On Merge to Main**:
1. Run dbt tests on production
2. Alert on failures

### Production Setup

For production, replace local Docker Compose with:
- **Google Cloud Composer** (managed Airflow), or
- **Kubernetes** with Helm chart

Set environment variables:
```bash
AIRFLOW__CORE__EXECUTOR=CeleryExecutor
AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=...
AIRFLOW_VAR_GCP_PROJECT_ID=...
```

---

## ğŸ“ˆ Monitoring

### Airflow UI
- DAG run history
- Task duration trends
- Error logs

### BigQuery
```sql
-- Data health check results
SELECT * FROM transport_analytics.fct_data_health_daily
ORDER BY check_date DESC;
```

### Slack Alerts (Optional)
Configure webhook in Airflow:
- Success notifications
- Failure alerts with error details

---

## ğŸ“ Skills Demonstrated

This project showcases:

**Data Engineering**
- API ingestion with pagination and retry
- Batch processing patterns
- Data lake architecture (Bronze/Silver/Gold)

**Cloud Infrastructure**
- Google Cloud Platform (BigQuery, GCS)
- Infrastructure as code
- Cost optimization

**Data Modeling**
- Dimensional modeling (Kimball)
- Star schema design
- Slowly changing dimensions

**Modern Tools**
- dbt for transformations
- Airflow for orchestration
- Docker for containerization

**Best Practices**
- Unit testing
- CI/CD pipelines
- Documentation
- Version control with Git
- Code reviews

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/my-feature`
3. Make changes with tests
4. Run quality checks: `make lint && make test`
5. Commit: `git commit -m "feat: add new feature"`
6. Push and create Pull Request

**Commit conventions**: `feat:`, `fix:`, `docs:`, `test:`, `refactor:`

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file

---

## ğŸ™ Acknowledgments

- **IDFM** (Ãle-de-France MobilitÃ©s) for open data
- **SNCF** for Transilien APIs
- **dbt Labs** for dbt Core
- **Apache Airflow** community

---

## ğŸ“ Contact

- ğŸ“§ Email: your.email@example.com
- ğŸ’¼ LinkedIn: [Your LinkedIn](https://linkedin.com/in/yourprofile)
- ğŸ› Issues: [GitHub Issues](https://github.com/your-username/idfm-analytics-dataops/issues)

---

**Built with â¤ï¸ to demonstrate modern data engineering practices**
